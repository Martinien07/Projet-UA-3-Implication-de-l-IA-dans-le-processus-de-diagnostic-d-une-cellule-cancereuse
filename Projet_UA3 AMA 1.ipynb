{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a6f58af",
   "metadata": {},
   "source": [
    "# Partie 4: Apprentissage supervisé (regression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a365ed",
   "metadata": {},
   "source": [
    "Dans le cadre de notre étude, nous avons étendu l’analyse du cancer du sein à une approche de régression, dont l’objectif n’est plus de prédire la nature bénigne ou maligne d’une masse, mais d’estimer une caractéristique morphologique continue directement liée à la structure de la tumeur. Parmi l’ensemble des mesures disponibles, la variable **area1**, correspondant à l’aire moyenne de la cellule ou de la masse observée, a été retenue comme cible pour la régression. \n",
    "\n",
    "\n",
    "Ce choix s’appuie sur plusieurs arguments :\n",
    "\n",
    "(1) l’aire constitue l’un des indicateurs les plus directs de la taille et du volume tumoral, des facteurs cliniquement associés à la gravité potentielle ;\n",
    "\n",
    "(2) il s’agit d’une variable continue, suffisamment bien distribuée pour permettre une modélisation robuste ;\n",
    "\n",
    "(3) elle présente une  corrélation avec des variables clés telles que le rayon, le périmètre ou la compacité, ce qui facilite la création de modèles prédictifs performants. \n",
    "\n",
    "En choisissant area1 comme cible, nous visons donc à construire un modèle de régression capable d’estimer la taille moyenne d'une lésion à partir des autres caractéristiques morphologiques, ce qui constitue un outil pertinent pour l’évaluation préliminaire du risque clinique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380e1d17",
   "metadata": {},
   "source": [
    "## I. Etude de la distribution de la variable cible area1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde78451",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, train, test, X_train, y_train, X_test, y_test = chargement_donnees_ucirepo(target='area1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e8ba20",
   "metadata": {},
   "source": [
    "En se referant aux études réalisées dans la partie , on a remarqué la variable area1 présente beaucoup de valeurs abérantes, donc nous utiliserons une transformation logarithmique afin de réduire l’échelle, atténuer les outliers et rendre les relations plus linéaires pour la régression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181198f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = df.drop(columns=['area1', 'Diagnosis'])\n",
    "y = df['area1']\n",
    "\n",
    "y_log = np.log1p(y)  # Transformation logarithmique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3192f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10305ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5927e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "y.plot(kind='box')\n",
    "plt.title(\"Distribution de la variable cible 'area1'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a33dd9c",
   "metadata": {},
   "source": [
    "on observe :\n",
    "\n",
    "min = 143\n",
    "\n",
    "max = 2501\n",
    "\n",
    "std = 352\n",
    "\n",
    "distribution très étalée → forte variance\n",
    "\n",
    "valeurs très différentes d’un patient à l’autre\n",
    " donc une erreur absolue (MAE) pénaliserait peu les grosses erreurs.\n",
    "\n",
    "\n",
    "la meilleure métrique pour cette cible est RMSE\n",
    "\n",
    "Parce que :\n",
    "\n",
    "* Grande plage de valeurs*\n",
    "\n",
    "Les prédictions éloignées de la vraie valeur doivent être plus pénalisées, ce que fait RMSE.\n",
    "\n",
    "* Sensible aux grandes erreurs*\n",
    "\n",
    "Si le modèle se trompe beaucoup pour des tumeurs très grandes, RMSE augmente fortement → ce qui est important pour une application médicale.\n",
    "\n",
    "* Cohérent avec une variable continue très dispersée*\n",
    "\n",
    "MAE donne la même importance à petites et grandes erreurs, ce qui n’est pas idéal ici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4215dc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.hist(bins=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f071cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_log.hist(bins=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5c4620",
   "metadata": {},
   "source": [
    "## II. Modélisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ac8429",
   "metadata": {},
   "source": [
    "Nous allons utiliser plusieurs algorithmes de régression afin de prédire la taille moyenne des zones cellulaires (area1) à partir des autres caractéristiques morphologiques des cellules.\n",
    "\n",
    "\n",
    "\n",
    "**Linear Regression**\n",
    "\n",
    "Utilisé comme modèle de base simple et interprétable, permettant de vérifier si la relation entre les caractéristiques cellulaires et area1 est principalement linéaire.\n",
    "\n",
    "\n",
    "**Ridge Regression**\n",
    "\n",
    "Choisi pour réduire la sensibilité aux variables corrélées entre elles grâce à la régularisation L2, ce qui stabilise les coefficients et améliore la robustesse.\n",
    "\n",
    "\n",
    "**Lasso Regression**\n",
    "\n",
    "Retenu car il réalise en plus une sélection automatique des variables via régularisation L1, utile pour simplifier le modèle si certaines variables influencent peu area1.\n",
    "\n",
    "\n",
    "**Decision Tree Regressor**\n",
    "\n",
    "Employé car il capture naturellement les relations non linéaires entre mesures cellulaires et area1, et il offre une structure explicable.\n",
    "\n",
    "\n",
    "**Random Forest Regressor**\n",
    "\n",
    "Sélectionné pour sa capacité à réduire l’overfitting en combinant plusieurs arbres et à modéliser des interactions complexes entre caractéristiques.\n",
    "\n",
    "\n",
    "**Gradient Boosting Regressor**\n",
    "\n",
    "Utilisé pour sa capacité à corriger progressivement les erreurs du modèle précédent et à détecter des patterns subtils influençant area1.\n",
    "\n",
    "\n",
    "**Bagging Regressor**\n",
    "\n",
    "Retenu car il améliore la stabilité et limite la variance des arbres de décision, surtout lorsque les données présentent des variations locales.\n",
    "\n",
    "\n",
    "**AdaBoost Regressor**\n",
    "\n",
    "Choisi pour son approche itérative qui met plus de poids sur les prédictions difficiles, ce qui peut améliorer la précision sur les mesures atypiques.\n",
    "\n",
    "\n",
    "**XGBoost Regressor**\n",
    "\n",
    "Employé pour ses performances élevées et ses mécanismes avancés de régularisation, particulièrement adaptés aux données tabulaires du WDBC.\n",
    "\n",
    "\n",
    "**LightGBM Regressor**\n",
    "\n",
    "Sélectionné pour sa rapidité d’entraînement et sa capacité à traiter efficacement les relations non linéaires dans des datasets numériques.\n",
    "\n",
    "\n",
    "**CatBoost Regressor**\n",
    "\n",
    "Privilégié pour sa stabilité, son faible besoin de réglages et sa gestion naturelle des relations complexes sans prétraitement lourd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5b691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor, GradientBoostingRegressor,\n",
    "    BaggingRegressor, AdaBoostRegressor\n",
    ")\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import scikit_posthocs as sp\n",
    "\n",
    "def modelisation_regression_area1_visualisee(X, y, cv_splits=5, log_activation=False):\n",
    "    \"\"\"\n",
    "    Compare plusieurs modèles de régression pour prédire 'area1' (RMSE).\n",
    "    - Ajoute une colonne 'cluster' issue d'un KMeans(2)\n",
    "    - Effectue une validation croisée manuelle (KFold)\n",
    "    - Calcule RMSE train/test, std, temps d'entraînement/prédiction\n",
    "    - Affiche 3 graphiques : RMSE train vs test (barres annotées), boxplot (variabilité),\n",
    "      temps d'entraînement vs prédiction (barres annotées)\n",
    "    - Calcule un score global (à maximiser) et renvoie le DataFrame des résultats et le meilleur modèle\n",
    "    \"\"\"\n",
    "\n",
    "    # copie des données pour sécurité\n",
    "    X = X.copy()\n",
    "    \n",
    "    if log_activation:\n",
    "        y_log = np.log1p(y)\n",
    "        y = y_log.copy().reset_index(drop=True)\n",
    "\n",
    "    y = y.copy().reset_index(drop=True)\n",
    "    X = X.reset_index(drop=True)\n",
    "\n",
    "    # ---- 1) Ajouter cluster (KMeans 2) ----\n",
    "    kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "    try:\n",
    "        X[\"cluster\"] = kmeans.fit_predict(X)\n",
    "    except Exception:\n",
    "        # si X contient des colonnes non numériques (peu probable ici), on essaie de sélectionner numériques\n",
    "        numeric_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "        X[\"cluster\"] = kmeans.fit_predict(X[numeric_cols])\n",
    "\n",
    "    # ---- 2) Prétraitement (StandardScaler sur colonnes numériques) ----\n",
    "    numeric_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "    preprocessor = ColumnTransformer(\n",
    "        [('num', StandardScaler(), numeric_features)],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    # ---- 3) Modèles de régression ----\n",
    "    models = {\n",
    "        \"Linear Regression\": LinearRegression(),\n",
    "        \"KNN\": KNeighborsRegressor(n_neighbors=5),\n",
    "        \"Decision Tree\": DecisionTreeRegressor(random_state=42),\n",
    "        \"Random Forest\": RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1),\n",
    "        \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, random_state=42),\n",
    "        \"Bagging\": BaggingRegressor(n_estimators=50, random_state=42),\n",
    "        \"AdaBoost\": AdaBoostRegressor(n_estimators=200, learning_rate=0.1, random_state=42),\n",
    "        \"XGBoost\": XGBRegressor(n_estimators=200, learning_rate=0.1, max_depth=5, random_state=42, verbosity=0),\n",
    "        \"LightGBM\": LGBMRegressor(n_estimators=200, learning_rate=0.1, random_state=42),\n",
    "        \"CatBoost\": CatBoostRegressor(iterations=200, learning_rate=0.1, depth=6, verbose=0, random_seed=42)\n",
    "    }\n",
    "\n",
    "    # ---- 4) Structures de stockage ----\n",
    "    rmse_train_means = {}\n",
    "    rmse_test_means = {}\n",
    "    rmse_test_stds = {}\n",
    "    train_times = {}\n",
    "    pred_times = {}\n",
    "    boxplot_values = {}\n",
    "\n",
    "    kf = KFold(n_splits=cv_splits, shuffle=True, random_state=42)\n",
    "    print(f\"\\n  Début de la validation croisée (CV={cv_splits})  \\n\")\n",
    "\n",
    "    # ---- 5) Boucle sur modèles, CV manuelle ----\n",
    "    for name, model in models.items():\n",
    "        print(f\"--- Modèle : {name} ---\")\n",
    "        pipeline = Pipeline([(\"preprocess\", preprocessor), (\"model\", model)])\n",
    "\n",
    "        rmse_train_folds = []\n",
    "        rmse_test_folds = []\n",
    "        times_train_folds = []\n",
    "        times_pred_folds = []\n",
    "\n",
    "        for fold_idx, (train_idx, test_idx) in enumerate(kf.split(X), 1):\n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "            # entraînement\n",
    "            t0 = time.time()\n",
    "            try:\n",
    "                pipeline.fit(X_train, y_train)\n",
    "            except Exception as e:\n",
    "                print(f\"  Erreur fit sur fold {fold_idx} pour {name} : {e}\")\n",
    "                # si échec, on ajoute NaN et continue\n",
    "                rmse_train_folds.append(np.nan)\n",
    "                rmse_test_folds.append(np.nan)\n",
    "                times_train_folds.append(np.nan)\n",
    "                times_pred_folds.append(np.nan)\n",
    "                continue\n",
    "            t1 = time.time()\n",
    "\n",
    "            # prédiction\n",
    "            t2 = time.time()\n",
    "            y_pred_train = pipeline.predict(X_train)\n",
    "            y_pred_test = pipeline.predict(X_test)\n",
    "            t3 = time.time()\n",
    "\n",
    "            # RMSE\n",
    "            rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "            rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "            rmse_train_folds.append(rmse_train)\n",
    "            rmse_test_folds.append(rmse_test)\n",
    "            times_train_folds.append(t1 - t0)\n",
    "            times_pred_folds.append(t3 - t2)\n",
    "\n",
    "            print(f\"  Fold {fold_idx} — RMSE_train: {rmse_train:.4f}, RMSE_test: {rmse_test:.4f}, \"\n",
    "                  f\"t_train: {t1-t0:.4f}s, t_pred: {t3-t2:.4f}s\")\n",
    "\n",
    "        # moyennes et std\n",
    "        rmse_train_means[name] = np.nanmean(rmse_train_folds)\n",
    "        rmse_test_means[name] = np.nanmean(rmse_test_folds)\n",
    "        rmse_test_stds[name] = np.nanstd(rmse_test_folds)\n",
    "        train_times[name] = np.nanmean(times_train_folds)\n",
    "        pred_times[name] = np.nanmean(times_pred_folds)\n",
    "        boxplot_values[name] = rmse_test_folds\n",
    "\n",
    "        print(f\"→ {name} Résumé : RMSE_train_mean = {rmse_train_means[name]:.4f}, \"\n",
    "              f\"RMSE_test_mean = {rmse_test_means[name]:.4f}, RMSE_test_std = {rmse_test_stds[name]:.4f}\\n\")\n",
    "\n",
    "    print(\"  Fin de la CV  \\n\")\n",
    "\n",
    "    # ---- 6) DataFrame récapitulatif ----\n",
    "    results_df = pd.DataFrame({\n",
    "        \"RMSE_train_mean\": rmse_train_means,\n",
    "        \"RMSE_test_mean\": rmse_test_means,\n",
    "        \"RMSE_test_std\": rmse_test_stds,\n",
    "        \"Train_time\": train_times,\n",
    "        \"Pred_time\": pred_times\n",
    "    })\n",
    "\n",
    "    # ---- 7) Score global (à maximiser). On minimise RMSE, donc on prend négatif.\n",
    "    alpha = 0.1   # pénalisation pour la variance\n",
    "    beta = 0.01   # pénalisation pour le temps de prédiction\n",
    "    results_df[\"Score_global\"] = - (results_df[\"RMSE_test_mean\"]\n",
    "                                    + alpha * results_df[\"RMSE_test_std\"]\n",
    "                                    + beta * results_df[\"Pred_time\"])\n",
    "\n",
    "    best_model_name = results_df[\"Score_global\"].idxmax()\n",
    "    print(\"  Meilleur modèle identifié  \")\n",
    "    print(results_df.loc[best_model_name])\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # ---- 8) Fonctions d'annotation ----\n",
    "    def annotate_bars(ax):\n",
    "        for bar in ax.patches:\n",
    "            h = bar.get_height()\n",
    "            ax.annotate(f\"{h:.3f}\",\n",
    "                        xy=(bar.get_x() + bar.get_width() / 2, h),\n",
    "                        xytext=(0, 5),\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "            \n",
    "\n",
    "    \n",
    "\n",
    "    # ---- 9) Graphiques ----\n",
    "    models_list = list(rmse_train_means.keys())\n",
    "    x = np.arange(len(models_list))\n",
    "    width = 0.35\n",
    "\n",
    "    # Graph 1 : RMSE train vs test\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    bars1 = ax.bar(x - width/2, [results_df.loc[m, \"RMSE_train_mean\"] for m in models_list], width, label=\"RMSE Train\")\n",
    "    bars2 = ax.bar(x + width/2, [results_df.loc[m, \"RMSE_test_mean\"] for m in models_list], width, label=\"RMSE Test\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(models_list, rotation=45, ha='right')\n",
    "    ax.set_ylabel(\"RMSE\")\n",
    "    ax.set_title(\"RMSE moyen (Train vs Test)\")\n",
    "    ax.legend()\n",
    "    annotate_bars(ax)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Graph 2 : Robustesse (boxplot RMSE test par fold)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    df_box = pd.DataFrame(boxplot_values)\n",
    "    sns.boxplot(data=df_box)\n",
    "    plt.title(\"Robustesse des modèles (variabilité RMSE-test sur les folds)\")\n",
    "    plt.ylabel(\"RMSE (test)\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Graph 3 : Temps entraînement vs prédiction\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    bars3 = ax.bar(x - width/2, [results_df.loc[m, \"Train_time\"] for m in models_list], width, label=\"Train time (s)\")\n",
    "    bars4 = ax.bar(x + width/2, [results_df.loc[m, \"Pred_time\"] for m in models_list], width, label=\"Pred time (s)\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(models_list, rotation=45, ha='right')\n",
    "    ax.set_title(\"Temps moyen d'entraînement et de prédiction\")\n",
    "    ax.legend()\n",
    "    annotate_bars(ax)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # ---- 10) Test de Friedman ----\n",
    "    print(\"\\n  Test de Friedman (comparaison de modèles sur les folds) \")\n",
    "\n",
    "    # On récupère les RMSE test par modèle sous forme de DataFrame (folds en ligne)\n",
    "    df_friedman = pd.DataFrame(boxplot_values)\n",
    "\n",
    "    # Test de Friedman\n",
    "    from scipy.stats import friedmanchisquare\n",
    "\n",
    "    friedman_stat, p_value = friedmanchisquare(*[df_friedman[col].dropna() for col in df_friedman.columns])\n",
    "\n",
    "    print(f\"Statistique de Friedman = {friedman_stat:.4f}\")\n",
    "    print(f\"P-value = {p_value:.4f}\")\n",
    "\n",
    "    # ---- 11) Si significatif → test post-hoc Nemenyi ----\n",
    "    if p_value < 0.05:\n",
    "        print(\"\\n→ Les performances diffèrent significativement (p < 0.05).\")\n",
    "        print(\"  Exécution du test post-hoc Nemenyi...\\n\")\n",
    "\n",
    "        \n",
    "\n",
    "        nemenyi_matrix = sp.posthoc_nemenyi_friedman(df_friedman)\n",
    "\n",
    "        print(\"===== Matrice de Nemenyi (p-values) =====\")\n",
    "        print(nemenyi_matrix)\n",
    "\n",
    "        # heatmap pour mieux visualiser\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(nemenyi_matrix, annot=True, cmap=\"viridis\")\n",
    "        plt.title(\"Post-hoc Nemenyi — Comparaison pairwise des modèles\")\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print(\"\\n→ Les performances ne diffèrent PAS significativement (p ≥ 0.05).\")\n",
    "\n",
    "\n",
    "    return results_df, best_model_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d80b833",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df, best = modelisation_regression_area1_visualisee(X_train, y_train, cv_splits=5)\n",
    "results_df\n",
    "print(\"Meilleur modèle :\", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443009db",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df, best = modelisation_regression_area1_visualisee(X_train, y_train, cv_splits=5, log_activation=True)\n",
    "print(\"Meilleur modèle :\", best)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0224a4",
   "metadata": {},
   "source": [
    "**Interprétation**\n",
    "\n",
    "Les résultats montrent que Gradient Boosting est le modèle le plus performant pour prédire area1. Il obtient le plus faible RMSE sur le test, une bonne stabilité, et un temps d’exécution raisonnable, ce qui en fait le meilleur compromis précision-robustesse.\n",
    "\n",
    "La régression linéaire arrive juste derrière : rapide et stable, mais un peu moins précise.\n",
    "Les modèles comme Decision Tree, Random Forest, XGBoost, LightGBM, CatBoost ou KNN présentent soit un fort surapprentissage, soit une variabilité trop élevée, soit des erreurs nettement supérieures.\n",
    "\n",
    "En résumé, Gradient Boosting est le meilleur choix global pour estimer la taille moyenne des cellules (area1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bc04dd",
   "metadata": {},
   "source": [
    "\n",
    "## II. Optimisation des modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2b5b86",
   "metadata": {},
   "source": [
    "\n",
    "Pour optimiser la performance de nos modèles, nous allons utilisé plusieurs techniques à savoir:\n",
    "* Le feature_engineering\n",
    "* L'optimisation des hyperparamètres via RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3958c2",
   "metadata": {},
   "source": [
    "### 1. feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944ee675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor, GradientBoostingRegressor,\n",
    "    BaggingRegressor, AdaBoostRegressor\n",
    ")\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import scikit_posthocs as sp\n",
    "\n",
    "def modelisation_regression_area1_visualisee(X, y, cv_splits=5, log_activation=False):\n",
    "    \"\"\"\n",
    "    Compare plusieurs modèles de régression pour prédire 'area1' (RMSE).\n",
    "    - Ajoute une colonne 'cluster' issue d'un KMeans(2)\n",
    "    - Effectue une validation croisée manuelle (KFold)\n",
    "    - Calcule RMSE train/test, std, temps d'entraînement/prédiction\n",
    "    - Affiche 3 graphiques : RMSE train vs test (barres annotées), boxplot (variabilité),\n",
    "      temps d'entraînement vs prédiction (barres annotées)\n",
    "    - Calcule un score global (à maximiser) et renvoie le DataFrame des résultats et le meilleur modèle\n",
    "    \"\"\"\n",
    "\n",
    "    # copie des données pour sécurité\n",
    "    X = X.copy()\n",
    "    \n",
    "    if log_activation:\n",
    "        y_log = np.log1p(y)\n",
    "        y = y_log.copy().reset_index(drop=True)\n",
    "\n",
    "    y = y.copy().reset_index(drop=True)\n",
    "    X = X.reset_index(drop=True)\n",
    "\n",
    "    # ---- 1) Ajouter cluster (KMeans 2) ----\n",
    "    kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "    try:\n",
    "        X[\"cluster\"] = kmeans.fit_predict(X)\n",
    "    except Exception:\n",
    "        # si X contient des colonnes non numériques (peu probable ici), on essaie de sélectionner numériques\n",
    "        numeric_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "        X[\"cluster\"] = kmeans.fit_predict(X[numeric_cols])\n",
    "\n",
    "    # ---- 2) Prétraitement (StandardScaler sur colonnes numériques) ----\n",
    "    numeric_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "    preprocessor = ColumnTransformer(\n",
    "        [('num', StandardScaler(), numeric_features)],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    # ---- 3) Modèles de régression ----\n",
    "    models = {\n",
    "        \"Linear Regression\": LinearRegression(),\n",
    "        \"KNN\": KNeighborsRegressor(n_neighbors=5),\n",
    "        \"Decision Tree\": DecisionTreeRegressor(random_state=42),\n",
    "        \"Random Forest\": RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1),\n",
    "        \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, random_state=42),\n",
    "        \"Bagging\": BaggingRegressor(n_estimators=50, random_state=42),\n",
    "        \"AdaBoost\": AdaBoostRegressor(n_estimators=200, learning_rate=0.1, random_state=42),\n",
    "        \"XGBoost\": XGBRegressor(n_estimators=200, learning_rate=0.1, max_depth=5, random_state=42, verbosity=0),\n",
    "        \"LightGBM\": LGBMRegressor(n_estimators=200, learning_rate=0.1, random_state=42),\n",
    "        \"CatBoost\": CatBoostRegressor(iterations=200, learning_rate=0.1, depth=6, verbose=0, random_seed=42)\n",
    "    }\n",
    "\n",
    "    # ---- 4) Structures de stockage ----\n",
    "    rmse_train_means = {}\n",
    "    rmse_test_means = {}\n",
    "    rmse_test_stds = {}\n",
    "    train_times = {}\n",
    "    pred_times = {}\n",
    "    boxplot_values = {}\n",
    "\n",
    "    kf = KFold(n_splits=cv_splits, shuffle=True, random_state=42)\n",
    "    print(f\"\\n  Début de la validation croisée (CV={cv_splits})  \\n\")\n",
    "\n",
    "    # ---- 5) Boucle sur modèles, CV manuelle ----\n",
    "    for name, model in models.items():\n",
    "        print(f\"--- Modèle : {name} ---\")\n",
    "        pipeline = Pipeline([(\"preprocess\", preprocessor), (\"model\", model)])\n",
    "\n",
    "        rmse_train_folds = []\n",
    "        rmse_test_folds = []\n",
    "        times_train_folds = []\n",
    "        times_pred_folds = []\n",
    "\n",
    "        for fold_idx, (train_idx, test_idx) in enumerate(kf.split(X), 1):\n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "            # entraînement\n",
    "            t0 = time.time()\n",
    "            try:\n",
    "                pipeline.fit(X_train, y_train)\n",
    "            except Exception as e:\n",
    "                print(f\"  Erreur fit sur fold {fold_idx} pour {name} : {e}\")\n",
    "                # si échec, on ajoute NaN et continue\n",
    "                rmse_train_folds.append(np.nan)\n",
    "                rmse_test_folds.append(np.nan)\n",
    "                times_train_folds.append(np.nan)\n",
    "                times_pred_folds.append(np.nan)\n",
    "                continue\n",
    "            t1 = time.time()\n",
    "\n",
    "            # prédiction\n",
    "            t2 = time.time()\n",
    "            y_pred_train = pipeline.predict(X_train)\n",
    "            y_pred_test = pipeline.predict(X_test)\n",
    "            t3 = time.time()\n",
    "\n",
    "            # RMSE\n",
    "            rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "            rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "            rmse_train_folds.append(rmse_train)\n",
    "            rmse_test_folds.append(rmse_test)\n",
    "            times_train_folds.append(t1 - t0)\n",
    "            times_pred_folds.append(t3 - t2)\n",
    "\n",
    "            print(f\"  Fold {fold_idx} — RMSE_train: {rmse_train:.4f}, RMSE_test: {rmse_test:.4f}, \"\n",
    "                  f\"t_train: {t1-t0:.4f}s, t_pred: {t3-t2:.4f}s\")\n",
    "\n",
    "        # moyennes et std\n",
    "        rmse_train_means[name] = np.nanmean(rmse_train_folds)\n",
    "        rmse_test_means[name] = np.nanmean(rmse_test_folds)\n",
    "        rmse_test_stds[name] = np.nanstd(rmse_test_folds)\n",
    "        train_times[name] = np.nanmean(times_train_folds)\n",
    "        pred_times[name] = np.nanmean(times_pred_folds)\n",
    "        boxplot_values[name] = rmse_test_folds\n",
    "\n",
    "        print(f\"→ {name} Résumé : RMSE_train_mean = {rmse_train_means[name]:.4f}, \"\n",
    "              f\"RMSE_test_mean = {rmse_test_means[name]:.4f}, RMSE_test_std = {rmse_test_stds[name]:.4f}\\n\")\n",
    "\n",
    "    print(\"  Fin de la CV  \\n\")\n",
    "\n",
    "    # ---- 6) DataFrame récapitulatif ----\n",
    "    results_df = pd.DataFrame({\n",
    "        \"RMSE_train_mean\": rmse_train_means,\n",
    "        \"RMSE_test_mean\": rmse_test_means,\n",
    "        \"RMSE_test_std\": rmse_test_stds,\n",
    "        \"Train_time\": train_times,\n",
    "        \"Pred_time\": pred_times\n",
    "    })\n",
    "\n",
    "    # ---- 7) Score global (à maximiser). On minimise RMSE, donc on prend négatif.\n",
    "    alpha = 0.1   # pénalisation pour la variance\n",
    "    beta = 0.01   # pénalisation pour le temps de prédiction\n",
    "    results_df[\"Score_global\"] = - (results_df[\"RMSE_test_mean\"]\n",
    "                                    + alpha * results_df[\"RMSE_test_std\"]\n",
    "                                    + beta * results_df[\"Pred_time\"])\n",
    "\n",
    "    best_model_name = results_df[\"Score_global\"].idxmax()\n",
    "    print(\"  Meilleur modèle identifié  \")\n",
    "    print(results_df.loc[best_model_name])\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # ---- 8) Fonctions d'annotation ----\n",
    "    def annotate_bars(ax):\n",
    "        for bar in ax.patches:\n",
    "            h = bar.get_height()\n",
    "            ax.annotate(f\"{h:.3f}\",\n",
    "                        xy=(bar.get_x() + bar.get_width() / 2, h),\n",
    "                        xytext=(0, 5),\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "            \n",
    "\n",
    "    \n",
    "\n",
    "    # ---- 9) Graphiques ----\n",
    "    models_list = list(rmse_train_means.keys())\n",
    "    x = np.arange(len(models_list))\n",
    "    width = 0.35\n",
    "\n",
    "    # Graph 1 : RMSE train vs test\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    bars1 = ax.bar(x - width/2, [results_df.loc[m, \"RMSE_train_mean\"] for m in models_list], width, label=\"RMSE Train\")\n",
    "    bars2 = ax.bar(x + width/2, [results_df.loc[m, \"RMSE_test_mean\"] for m in models_list], width, label=\"RMSE Test\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(models_list, rotation=45, ha='right')\n",
    "    ax.set_ylabel(\"RMSE\")\n",
    "    ax.set_title(\"RMSE moyen (Train vs Test)\")\n",
    "    ax.legend()\n",
    "    annotate_bars(ax)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Graph 2 : Robustesse (boxplot RMSE test par fold)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    df_box = pd.DataFrame(boxplot_values)\n",
    "    sns.boxplot(data=df_box)\n",
    "    plt.title(\"Robustesse des modèles (variabilité RMSE-test sur les folds)\")\n",
    "    plt.ylabel(\"RMSE (test)\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Graph 3 : Temps entraînement vs prédiction\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    bars3 = ax.bar(x - width/2, [results_df.loc[m, \"Train_time\"] for m in models_list], width, label=\"Train time (s)\")\n",
    "    bars4 = ax.bar(x + width/2, [results_df.loc[m, \"Pred_time\"] for m in models_list], width, label=\"Pred time (s)\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(models_list, rotation=45, ha='right')\n",
    "    ax.set_title(\"Temps moyen d'entraînement et de prédiction\")\n",
    "    ax.legend()\n",
    "    annotate_bars(ax)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # ---- 10) Test de Friedman ----\n",
    "    print(\"\\n  Test de Friedman (comparaison de modèles sur les folds) \")\n",
    "\n",
    "    # On récupère les RMSE test par modèle sous forme de DataFrame (folds en ligne)\n",
    "    df_friedman = pd.DataFrame(boxplot_values)\n",
    "\n",
    "    # Test de Friedman\n",
    "    from scipy.stats import friedmanchisquare\n",
    "\n",
    "    friedman_stat, p_value = friedmanchisquare(*[df_friedman[col].dropna() for col in df_friedman.columns])\n",
    "\n",
    "    print(f\"Statistique de Friedman = {friedman_stat:.4f}\")\n",
    "    print(f\"P-value = {p_value:.4f}\")\n",
    "\n",
    "    # ---- 11) Si significatif → test post-hoc Nemenyi ----\n",
    "    if p_value < 0.05:\n",
    "        print(\"\\n→ Les performances diffèrent significativement (p < 0.05).\")\n",
    "        print(\"  Exécution du test post-hoc Nemenyi...\\n\")\n",
    "\n",
    "        \n",
    "\n",
    "        nemenyi_matrix = sp.posthoc_nemenyi_friedman(df_friedman)\n",
    "\n",
    "        print(\"===== Matrice de Nemenyi (p-values) =====\")\n",
    "        print(nemenyi_matrix)\n",
    "\n",
    "        # heatmap pour mieux visualiser\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(nemenyi_matrix, annot=True, cmap=\"viridis\")\n",
    "        plt.title(\"Post-hoc Nemenyi — Comparaison pairwise des modèles\")\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print(\"\\n→ Les performances ne diffèrent PAS significativement (p ≥ 0.05).\")\n",
    "\n",
    "\n",
    "    return results_df, best_model_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74596fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_f_eng, y_f_eng=add_regression_features_area1(train)\n",
    "results_df, best = modelisation_regression_area1_visualisee(X_f_eng, y_train, cv_splits=5, log_activation=True)\n",
    "print(\"Meilleur modèle :\", best)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6138e9a7",
   "metadata": {},
   "source": [
    "**Interprétation**\n",
    "\n",
    "Le Gradient Boosting est le meilleur modèle : plus faible RMSE test (0.0218), stabilité correcte (std 0.0052) et temps de prédiction très faible (0.009 s).\n",
    "XGBoost suit de près (RMSE 0.0238) mais est un peu moins stable et plus lent.\n",
    "Linear Regression et Random Forest offrent des performances acceptables (RMSE ≈ 0.028) et des temps très rapides pour la prédiction.\n",
    "Le Decision Tree apprend trop parfaitement (RMSE train = 0) mais généralise mal (test 0.037), signe d’overfitting.\n",
    "Les moins bons modèles sont LightGBM, CatBoost et AdaBoost (RMSE > 0.038), avec une variabilité plus forte et des temps d’entraînement élevés.\n",
    "\n",
    "A partir du test de Friedman on peut dire : \n",
    "\n",
    "Les modèles Linear Regression, Decision Tree, Random Forest, Bagging, AdaBoost, Gradient Boosting et XGBoost sont tous statistiquement équivalents, offrant les meilleures performances.\n",
    "KNN est significativement moins performant.\n",
    "LightGBM et CatBoost ne sont pas significativement meilleurs que KNN et restent derrière les meilleurs modèles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415f3340",
   "metadata": {},
   "source": [
    "## 3. Optimisation des hyperparamètres via RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fb2fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor, GradientBoostingRegressor,\n",
    "    BaggingRegressor, AdaBoostRegressor\n",
    ")\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# --- Scorer RMSE (Sklearn demande un score → on met négatif) ---\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "rmse_scorer = make_scorer(rmse, greater_is_better=False)\n",
    "\n",
    "\n",
    "def hyperparam_optimization_regression(X, y, cv_splits=5, n_iter=40, random_state=42):\n",
    "    \"\"\"\n",
    "    Optimisation des hyperparamètres pour plusieurs modèles de régression\n",
    "    (cible : area1) avec RandomizedSearchCV utilisant le RMSE.\n",
    "    \"\"\"\n",
    "\n",
    "    numeric_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        [('num', StandardScaler(), numeric_features)],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    # --- Modèles et grilles d’hyperparamètres ---\n",
    "    model_params = {\n",
    "\n",
    "        \"Ridge\": {\n",
    "            \"model\": Ridge(),\n",
    "            \"params\": {\n",
    "                \"model__alpha\": [0.01, 0.1, 1, 10, 50, 100]\n",
    "            }\n",
    "        },\n",
    "\n",
    "        \"Lasso\": {\n",
    "            \"model\": Lasso(),\n",
    "            \"params\": {\n",
    "                \"model__alpha\": [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "            }\n",
    "        },\n",
    "\n",
    "        \"Decision Tree\": {\n",
    "            \"model\": DecisionTreeRegressor(random_state=random_state),\n",
    "            \"params\": {\n",
    "                \"model__max_depth\": [None, 3, 5, 7, 10],\n",
    "                \"model__min_samples_split\": [2, 5, 10],\n",
    "                \"model__min_samples_leaf\": [1, 2, 5]\n",
    "            }\n",
    "        },\n",
    "\n",
    "        \"Random Forest\": {\n",
    "            \"model\": RandomForestRegressor(random_state=random_state),\n",
    "            \"params\": {\n",
    "                \"model__n_estimators\": [200, 400, 600],\n",
    "                \"model__max_depth\": [None, 5, 10],\n",
    "                \"model__min_samples_split\": [2, 5, 10]\n",
    "            }\n",
    "        },\n",
    "\n",
    "        \"Gradient Boosting\": {\n",
    "            \"model\": GradientBoostingRegressor(random_state=random_state),\n",
    "            \"params\": {\n",
    "                \"model__n_estimators\": [100, 200, 400],\n",
    "                \"model__learning_rate\": [0.01, 0.05, 0.1],\n",
    "                \"model__max_depth\": [2, 3, 4]\n",
    "            }\n",
    "        },\n",
    "\n",
    "        \"Bagging\": {\n",
    "            \"model\": BaggingRegressor(random_state=random_state),\n",
    "            \"params\": {\n",
    "                \"model__n_estimators\": [50, 100, 200],\n",
    "                \"model__max_samples\": [0.5, 0.7, 1.0]\n",
    "            }\n",
    "        },\n",
    "\n",
    "        \"AdaBoost\": {\n",
    "            \"model\": AdaBoostRegressor(random_state=random_state),\n",
    "            \"params\": {\n",
    "                \"model__n_estimators\": [100, 200, 400],\n",
    "                \"model__learning_rate\": [0.01, 0.1, 0.5, 1.0]\n",
    "            }\n",
    "        },\n",
    "\n",
    "        \"XGBoost\": {\n",
    "            \"model\": XGBRegressor(random_state=random_state),\n",
    "            \"params\": {\n",
    "                \"model__n_estimators\": [200, 400, 600],\n",
    "                \"model__learning_rate\": [0.01, 0.05, 0.1],\n",
    "                \"model__max_depth\": [3, 5, 7]\n",
    "            }\n",
    "        },\n",
    "\n",
    "        \"LightGBM\": {\n",
    "            \"model\": LGBMRegressor(random_state=random_state),\n",
    "            \"params\": {\n",
    "                \"model__n_estimators\": [200, 400, 600],\n",
    "                \"model__learning_rate\": [0.01, 0.05, 0.1],\n",
    "                \"model__num_leaves\": [20, 31, 50]\n",
    "            }\n",
    "        },\n",
    "\n",
    "        \"CatBoost\": {\n",
    "            \"model\": CatBoostRegressor(verbose=0, random_seed=random_state),\n",
    "            \"params\": {\n",
    "                \"model__depth\": [4, 6, 8],\n",
    "                \"model__iterations\": [200, 400, 600],\n",
    "                \"model__learning_rate\": [0.01, 0.05, 0.1]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    best_models = {}\n",
    "    best_params = {}\n",
    "\n",
    "    # --- Boucle d’optimisation ---\n",
    "    for name, mp in model_params.items():\n",
    "        print(f\"\\n  Optimisation de {name}  \")\n",
    "\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocess', preprocessor),\n",
    "            ('model', mp[\"model\"])\n",
    "        ])\n",
    "\n",
    "        search = RandomizedSearchCV(\n",
    "            estimator=pipeline,\n",
    "            param_distributions=mp[\"params\"],\n",
    "            scoring=rmse_scorer,\n",
    "            n_iter=n_iter,\n",
    "            cv=cv_splits,\n",
    "            verbose=1,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        search.fit(X, y)\n",
    "\n",
    "        best_models[name] = search.best_estimator_\n",
    "        best_params[name] = search.best_params_\n",
    "\n",
    "        print(\"→ Best RMSE :\", round(-search.best_score_, 4))\n",
    "        print(\"→ Best params :\", search.best_params_)\n",
    "\n",
    "    return best_models, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3da1cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models, best_params = hyperparam_optimization_regression(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d75154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244129a1",
   "metadata": {},
   "source": [
    "## Evaluation sur les données de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8c45b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, BaggingRegressor, AdaBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "def modele_regression_train_test(X_train, y_train, X_test, y_test, log_activation=False):\n",
    "    \"\"\"\n",
    "    Entraîne plusieurs modèles sur X_train/y_train, prédit sur X_test/y_test.\n",
    "    - Ajoute une colonne 'cluster' issue d'un KMeans(2)\n",
    "    - Standardisation des colonnes numériques\n",
    "    - Affiche les métriques R2, MAE, RMSE sous forme de graphiques à barres\n",
    "    - Affiche les temps d'entraînement et de prédiction\n",
    "    \"\"\"\n",
    "    # Copie des données pour sécurité\n",
    "    X_train = X_train.copy()\n",
    "    X_test = X_test.copy()\n",
    "    y_train = y_train.copy()\n",
    "    y_test = y_test.copy()\n",
    "    \n",
    "    if log_activation:\n",
    "        y_train = np.log1p(y_train)\n",
    "        y_test = np.log1p(y_test)\n",
    "\n",
    "    # ---- 1) Ajouter cluster (KMeans 2) ----\n",
    "    kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "    try:\n",
    "        X_train[\"cluster\"] = kmeans.fit_predict(X_train)\n",
    "        X_test[\"cluster\"] = kmeans.predict(X_test)\n",
    "    except Exception:\n",
    "        numeric_cols = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "        X_train[\"cluster\"] = kmeans.fit_predict(X_train[numeric_cols])\n",
    "        X_test[\"cluster\"] = kmeans.predict(X_test[numeric_cols])\n",
    "\n",
    "    # ---- 2) Prétraitement (StandardScaler sur colonnes numériques) ----\n",
    "    numeric_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "    preprocessor = ColumnTransformer(\n",
    "        [('num', StandardScaler(), numeric_features)],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    # ---- 3) Modèles avec hyperparamètres ----\n",
    "    models = {\n",
    "        \"Ridge\": Ridge(alpha=0.1),\n",
    "        \"Lasso\": Lasso(alpha=0.1),\n",
    "        \"Decision Tree\": DecisionTreeRegressor(min_samples_split=2, min_samples_leaf=1, max_depth=7, random_state=42),\n",
    "        \"Random Forest\": RandomForestRegressor(n_estimators=400, min_samples_split=2, max_depth=None, random_state=42, n_jobs=-1),\n",
    "        \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=200, max_depth=2, learning_rate=0.1, random_state=42),\n",
    "        \"Bagging\": BaggingRegressor(n_estimators=200, max_samples=1.0, random_state=42),\n",
    "        \"AdaBoost\": AdaBoostRegressor(n_estimators=200, learning_rate=1.0, random_state=42),\n",
    "        \"XGBoost\": XGBRegressor(n_estimators=600, max_depth=3, learning_rate=0.05, random_state=42, verbosity=0),\n",
    "        \"LightGBM\": LGBMRegressor(num_leaves=20, n_estimators=200, learning_rate=0.05, random_state=42),\n",
    "        \"CatBoost\": CatBoostRegressor(iterations=600, learning_rate=0.1, depth=4, verbose=0, random_seed=42)\n",
    "    }\n",
    "\n",
    "    # ---- 4) Stockage métriques ----\n",
    "    results = {\n",
    "        \"R2\": {},\n",
    "        \"MAE\": {},\n",
    "        \"RMSE\": {},\n",
    "        \"Train_time\": {},\n",
    "        \"Pred_time\": {}\n",
    "    }\n",
    "\n",
    "    # ---- 5) Boucle sur modèles ----\n",
    "    for name, model in models.items():\n",
    "        pipeline = Pipeline([(\"preprocess\", preprocessor), (\"model\", model)])\n",
    "        print(f\"--- Entraînement : {name} ---\")\n",
    "\n",
    "        # Entraînement\n",
    "        t0 = time.time()\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        t1 = time.time()\n",
    "\n",
    "        # Prédiction\n",
    "        t2 = time.time()\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        t3 = time.time()\n",
    "\n",
    "        # Calcul métriques\n",
    "        results[\"R2\"][name] = r2_score(y_test, y_pred)\n",
    "        results[\"MAE\"][name] = mean_absolute_error(y_test, y_pred)\n",
    "        results[\"RMSE\"][name] = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        results[\"Train_time\"][name] = t1 - t0\n",
    "        results[\"Pred_time\"][name] = t3 - t2\n",
    "\n",
    "        print(f\"R2={results['R2'][name]:.4f}, MAE={results['MAE'][name]:.4f}, RMSE={results['RMSE'][name]:.4f}, \"\n",
    "              f\"t_train={results['Train_time'][name]:.3f}s, t_pred={results['Pred_time'][name]:.3f}s\")\n",
    "\n",
    "    # ---- 6) DataFrame récapitulatif ----\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # ---- 7) Graphiques ----\n",
    "    def plot_metric(metric, title):\n",
    "        plt.figure(figsize=(12,6))\n",
    "        sns.barplot(x=results_df.index, y=results_df[metric], palette=\"viridis\")\n",
    "        plt.title(title)\n",
    "        plt.ylabel(metric)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    plot_metric(\"R2\", \"R² des modèles sur X_test\")\n",
    "    plot_metric(\"MAE\", \"MAE des modèles sur X_test\")\n",
    "    plot_metric(\"RMSE\", \"RMSE des modèles sur X_test\")\n",
    "\n",
    "    # Graphique temps\n",
    "    plt.figure(figsize=(12,6))\n",
    "    x = np.arange(len(results_df))\n",
    "    width = 0.35\n",
    "    plt.bar(x - width/2, results_df[\"Train_time\"], width, label=\"Train time (s)\")\n",
    "    plt.bar(x + width/2, results_df[\"Pred_time\"], width, label=\"Pred time (s)\")\n",
    "    plt.xticks(x, results_df.index, rotation=45, ha='right')\n",
    "    plt.ylabel(\"Temps (s)\")\n",
    "    plt.title(\"Temps d'entraînement et de prédiction\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c4d2e3",
   "metadata": {},
   "source": [
    "**Interprétation**\n",
    "\n",
    "\n",
    "Random Forest : meilleur compromis précision/robustesse. R² très élevé (0.9981), MAE et RMSE les plus bas, temps d’entraînement/prédiction corrects.\n",
    "\n",
    "Bagging : R² proche de Random Forest (0.9979), MAE/RMSE légèrement plus élevés, mais temps d’entraînement plus long (2.7 s).\n",
    "\n",
    "XGBoost : R² = 0.9967, MAE/RMSE un peu plus hauts que Random Forest, temps d’entraînement 1.6 s, rapide en prédiction.\n",
    "\n",
    "Gradient Boosting : R² correct (0.9964), MAE/RMSE un peu plus élevés, temps d’entraînement 1 s, prédiction très rapide.\n",
    "\n",
    "Decision Tree, Ridge, Lasso : très rapides à entraîner, R² déjà très bon (>0.995), mais erreurs légèrement supérieures à Random Forest/Bagging/XGBoost.\n",
    "\n",
    "AdaBoost, CatBoost, LightGBM : R² et erreurs moins bonnes, à éviter pour ce dataset malgré certains temps d’entraînement raisonnables.\n",
    "\n",
    "\n",
    "\n",
    "Conclusion : En trouvant les meilleurs hyperparamètres , le Random Forest devient  le meilleur modèle pour la tache , suivi de près par Bagging et XGBoost pour ceux qui acceptent un léger compromis sur les performances, le Gradient Boosting demeure une meilleure option."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1a84a7",
   "metadata": {},
   "source": [
    "Nous allons faire toutes ces étapes dans une seule fonction\n",
    "\n",
    "Elle prend en entrée notre dataframe et nous retourne les différentes parties ou données dont nous aurons besoins pour l'entrainnement, le test, et l'interprétabilité."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fec427e",
   "metadata": {},
   "source": [
    "# Partie 5  Discussions finales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cf4a68",
   "metadata": {},
   "source": [
    "**Synthèse de l’étude sur la base AMA (UCI)**\n",
    "\n",
    "Cette étude a pour objectif de prédire deux aspects essentiels à partir de données tabulaires extraites d’images PAAF :\n",
    "\n",
    "Le diagnostic (benin/malin)\n",
    "\n",
    "La valeur de l’aire area1 des noyaux cellulaires.\n",
    "\n",
    "Elle explore différents modèles classiques de machine learning pour identifier ceux qui offrent le meilleur compromis entre précision, robustesse et rapidité, sans recourir à des modèles complexes type CNN, car les caractéristiques pertinentes ont été pré-extraites.\n",
    "\n",
    "**Partie 1 : Prédiction du diagnostic**\n",
    "\n",
    "\n",
    "\n",
    "Objectif : Identifier correctement les masses mammaires bénignes et malignes.\n",
    "\n",
    "Meilleur modèle : Régression Logistique\n",
    "\n",
    " F1-test = 963855,  F1-train = 0.967552 donc pas d’overfitting.\n",
    "\n",
    "Très rapide : entraînement 0.03 s, prédiction 0.01 s.\n",
    "\n",
    "Capacité élevée à détecter les cas positifs (malins).\n",
    "\n",
    "Modèles complexes (Random Forest, Gradient Boosting, XGBoost, LightGBM)\n",
    "\n",
    "Ajustement parfait sur le train (1.00), mais moins performant sur test (≈0.92–0.94).\n",
    "\n",
    "Sont sensibles au bruit et au surapprentissage.\n",
    "\n",
    "Conclusion opérationnelle : La Régression Logistique est le modèle le plus sûr et stable pour un usage réel, combinant rapidité et détection fiable des cas positifs.\n",
    "\n",
    "**Partie 2 : Prédiction de area1**\n",
    "\n",
    "\n",
    "\n",
    "Objectif : Prédire précisément l’aire des noyaux cellulaires (area1).\n",
    "\n",
    "Meilleur modèle : Random Forest\n",
    "\n",
    "R² = 0.9981, MAE = 7.55, RMSE = 14.90 → meilleure précision et robustesse.\n",
    "\n",
    "Temps d’entraînement/prédiction corrects.\n",
    "\n",
    "Autres modèles performants :\n",
    "\n",
    "Bagging : R² = 0.9979, MAE/RMSE légèrement supérieurs, plus lent à entraîner.\n",
    "\n",
    "XGBoost : R² = 0.9967, MAE/RMSE un peu plus élevés, rapide en prédiction.\n",
    "\n",
    "Modèles à éviter : LightGBM, AdaBoost, CatBoost → erreurs plus grandes malgré certains temps d’entraînement corrects.\n",
    "\n",
    "Conclusion : Random Forest offre le meilleur compromis pour la prédiction numérique de area1.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Combinaison des deux parties : bénéfices**\n",
    "\n",
    "Prédiction diagnostique + prédiction d’aire : combiner les deux approches permet :\n",
    "\n",
    "D’avoir un diagnostic fiable et une quantification précise des caractéristiques tumorales.\n",
    "\n",
    "De prioriser les patients à risque élevé en fonction de la taille/aire des noyaux et du diagnostic.\n",
    "\n",
    "D’orchestrer les modèles :\n",
    "\n",
    "Régression Logistique pour le diagnostic rapide et stable.\n",
    "\n",
    "Random Forest (ou Bagging/XGBoost selon contraintes de temps) pour prédiction précise des valeurs continues.\n",
    "\n",
    "Cette approche hybride maximise la robustesse et l’efficacité opérationnelle.\n",
    "\n",
    "\n",
    "\n",
    "**Conclusion générale**\n",
    "\n",
    "L’étude montre que des modèles classiques bien paramétrés peuvent surpasser les modèles complexes sur ce type de données tabulaires, extraites d’images médicales.\n",
    "\n",
    "Régression Logistique → meilleure pour le diagnostic, rapide et stable.\n",
    "\n",
    "Random Forest → meilleure pour la prédiction de area1, précise et robuste.\n",
    "\n",
    "La combinaison des deux modèles permet d’avoir à la fois précision diagnostique et quantification des caractéristiques tumorales, ce qui est très utile pour la prise de décision clinique et la priorisation des cas.\n",
    "\n",
    "Cette approche illustre que l’orchestration de modèles spécialisés selon la nature de la tâche (classification vs régression) est une stratégie efficace pour tirer le meilleur parti des données."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816d0b1e",
   "metadata": {},
   "source": [
    "## **Conclusion  du projet**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0a84c4",
   "metadata": {},
   "source": [
    "Ce projet nous a permis de mettre en pratique les concepts liés aux réseaux de neurones, et plus particulièrement à l’utilisation d’un autoencodeur. Il a constitué une véritable opportunité pour nous initier à la conception autonome de modèles neuronaux, en développant des compétences essentielles telles que l’anticipation des problèmes, la gestion des erreurs et l’amélioration de nos méthodes de recherche grâce à l’exploration de diverses sources et documentations techniques.\n",
    "\n",
    "Nous tenons à exprimer notre gratitude à notre enseignant pour cette opportunité, ainsi que pour le temps et l’énergie précieux consacrés à la préparation et à l’encadrement de ce projet.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
